### Day 1 (2025.12.22)

- 初始化项目结构
- 对接 DeepSeek API
- 数据整理，放入 SQLite 数据库
- [思考] 新闻数据是纯 html 无法直接用统计学方法进行文本分析，如果先清理再处理会耗费大量时间，在这里可以直接利用大模型进行信息抽取

总结：
- 已经完成初步的数据处理

---

### Day 2 (2025.12.23)

- 接下来要做的：
    - 先做一个可视化决策面板，来获取数据的全貌
    - 实现 trading agent 完成基本 pipeline
    - 回测引擎
    - 对比实验
    - 微调

- 突然想到情绪分析这一块应该用更严谨的方式来回测，而不是直接用 prompt
- nof1.ai 很好的证明了光靠 LLM 的 Trading Agent 是不可靠的，必须有足够的 reasoning 防止大模型幻觉
- 遇到了一个问题，发现数据其实有很多缺失

解决方案：保证 features table 的完整性

- [思考] 考虑未来用 column based database 来存储因子？

- 现在代码有点多，适合重构

- 在仅一周周期且缺乏人工标注数据集的前提下，进行参数级微调（SFT / RLHF / PEFT）在工程和方法论上都不合理
- 考虑 LLM Agent 接口尚未确定，先做一个 Naive
- 发现数据量实在太少了，基本无法有效训练
- 根据试验 `1.2` 可以看出来普通的策略在上涨大盘中倾向于持有，这么说在多持仓模型下如何调配持仓变得更加重要了
- 本来想用一个追涨杀跌的策略来测试风险控制模块，结果发现它是赚的最多的，当然它在各类风险指标它也是最危险的

总结：
- 做了一些代码结构上的优化，项目框架基本成型，测试了一些朴素策略，多持仓 Agent 结果不尽人意

- 优化方向（LLM Agent 思路）：
    - 之后的研究主要专注单股票策略，利用别的股票的信息交叉验证
    - 用其中 10 支股票训练一个 predict 模型（例如 LSTM）先进行测试作为 baseline
    - 接着在原本的 predict 模型上融合大模型来修正决策，类似 LLM 增强框架
    - 绘制系统结构图
    - 起草 REPORT
    - 增加 agent 横向对比

---

### Day 3

- 今天训练 LSTM 不出意外的几乎没有预测能力，但是不妨碍在上面使用 LLM 增强框架
- 完成了 Cited-RAG 的构建
- 用户问题 → 检索 → 证据构建 → LLM生成 → 引用规范化 → 引用清理 → 验证 → 最终答案
- 测试将原本的裸 LLM 模块改成 RAG 模块，测试性能提升

总结：
- 尝试直接用 RAG 来做一个简单的问答 Agent 但是觉得技术路线有问题，实在不太行，因为没有用 LLM 做 Agent 的经验，还是需要调研一下

---

### Day 4

- 中午开始研究如何用 LLM 做 Agent：
    - 新闻驱动：通过新闻信号来做判断
    - Reflection-driven：通过错误来自我纠错
    - Debate Driven：多头分析师、空头分析师、风控官
      解读：感觉有点类似 multi agent
    - RL-driven
      实现难度太高了，但是 state、action 的思路依旧可以参考

- 因为 LLM 调度开销实在太大，决定用 reflection driven model 对整体仓位进行优化
- 同时因为 multi asset model 收益率一直非常低，优化空间十足，故选择这个作为实验对象

---

### Day 5

Day off

---

### Day 6

- 设计 fewshots 对提示词作优化测试
- 生成 并且从 `volatile_days_20stocks_10days.json` 里挑了一些高波动日子的案例，手动标注了一些决策样例
- 测试了几种不同的 few-shot 格式，发现把决策理由写清楚比单纯给输入输出效果好
- 跑了几轮实验，few-shot 确实能提升一点稳定性，但提升幅度有限，可能还是 prompt 设计的问题

- [思考] 关于蒸馏的优化：
    - 之前想过直接用 LLM 的输出作为监督信号来训练小模型，但发现 LLM 的输出本身就不够稳定
    - 先把手头的实验跑完再说

总结：
- few-shot 优化有一定效果，但还不够理想，可能需要更细致的 prompt engineering

---

### Day 7

- 开始撰写报告
- 整理实验数据
- 未来可以用 蒸馏 来对 fewshots 优化?